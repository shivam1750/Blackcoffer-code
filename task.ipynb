{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import nltk.corpus as corpus\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in a:\\project\\ml_project_01\\venv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>50921.0</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>51382.8</td>\n",
       "      <td>https://insights.blackcoffer.com/coronavirus-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>51844.6</td>\n",
       "      <td>https://insights.blackcoffer.com/what-are-the-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>52306.4</td>\n",
       "      <td>https://insights.blackcoffer.com/marketing-dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>52768.2</td>\n",
       "      <td>https://insights.blackcoffer.com/continued-dem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URL_ID                                                URL\n",
       "0      123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1      321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2     2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3     4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4      432.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "..       ...                                                ...\n",
       "109  50921.0  https://insights.blackcoffer.com/coronavirus-i...\n",
       "110  51382.8  https://insights.blackcoffer.com/coronavirus-i...\n",
       "111  51844.6  https://insights.blackcoffer.com/what-are-the-...\n",
       "112  52306.4  https://insights.blackcoffer.com/marketing-dri...\n",
       "113  52768.2  https://insights.blackcoffer.com/continued-dem...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Input.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in a:\\project\\ml_project_01\\venv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in a:\\project\\ml_project_01\\venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error accessing https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Data extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_title = soup.find('title').get_text()\n",
    "        article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "        return article_title, article_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "input_df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    article_title, article_text = extract_article_text(url)\n",
    "    if article_text is not None:\n",
    "        with open(f'extracted_test/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"{article_title}\\n\\n{article_text}\")\n",
    "\n",
    "print(\"Data extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in a:\\project\\ml_project_01\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: textblob in a:\\project\\ml_project_01\\venv\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: pandas in a:\\project\\ml_project_01\\venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: click in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in a:\\project\\ml_project_01\\venv\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk textblob pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    # Remove punctuations\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    # Simple syllable count (excluding some exceptions)\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    word = word.lower()\n",
    "\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "\n",
    "    if word.endswith(\"le\"):\n",
    "        count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variables(text):\n",
    "    words = clean_text(text)\n",
    "    num_words = len(words)\n",
    "    num_sentences = len(sent_tokenize(text))\n",
    "    num_complex_words = sum(1 for word in words if count_syllables(word) > 2)\n",
    "\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    percentage_complex_words = num_complex_words / num_words\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    complex_word_count = num_complex_words\n",
    "    syllable_per_word = sum(count_syllables(word) for word in words) / num_words\n",
    "\n",
    "    # Sentiment Analysis using TextBlob\n",
    "    blob = TextBlob(text)\n",
    "    positive_score = len([s for s in blob.words if s.lower() in positive_words])\n",
    "    negative_score = len([s for s in blob.words if s.lower() in negative_words])\n",
    "\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (num_words + 0.000001)\n",
    "\n",
    "    # Personal Pronouns count using regex\n",
    "    personal_pronouns = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', text, flags=re.IGNORECASE))\n",
    "\n",
    "    avg_word_length = sum(len(word) for word in words) / num_words\n",
    "\n",
    "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "            avg_sentence_length, percentage_complex_words, fog_index,\n",
    "            avg_words_per_sentence, complex_word_count, num_words,\n",
    "            syllable_per_word, personal_pronouns, avg_word_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in a:\\project\\ml_project_01\\venv\\lib\\site-packages (5.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive-words.txt', 'r', encoding='cp1252') as file:\n",
    "    positive_words = set(file.read().splitlines())\n",
    "\n",
    "with open('negative-words.txt', 'r', encoding='cp1252') as file:\n",
    "    negative_words = set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the output\n",
    "output_df = pd.DataFrame(columns=['URL_ID'] + [\n",
    "    'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
    "    'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found for URL_ID: 11668.0\n",
      "File not found for URL_ID: 17671.4\n",
      "Data analysis completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "results = []\n",
    "\n",
    "# Iterate over rows in the input dataframe\n",
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    text_file_name = f\"extracted_test/{url_id}.txt\"  # Adjust the path to the extracted folder\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(text_file_name):\n",
    "        # Read the text file\n",
    "        with open(text_file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Calculate variables\n",
    "        variables = calculate_variables(text)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({'URL_ID': url_id, **dict(zip(output_df.columns[1:], variables))})\n",
    "    else:\n",
    "        print(f\"File not found for URL_ID: {url_id}\")\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the output DataFrame to an Excel file\n",
    "output_df.to_excel('Output_final_analysis.xlsx', index=False)\n",
    "\n",
    "print(\"Data analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
